# V1.0 Design: Ruthlessly Simple Memory System

**Date**: 2026-01-31  
**Status**: READY FOR IMPLEMENTATION  
**Complexity Target**: ~500 lines of code  
**Timeline**: 1-3 days implementation  

---

## 1. Executive Summary

### What V1.0 Delivers

**Core Capability**: Semantic memory storage and retrieval with agent identity namespacing.

```python
# Store a memory
memory_id = memory_store("Discussed PostgreSQL migration strategy for users table")

# Search semantically
results = memory_search("what did we decide about databases?")
# Returns: ["Discussed PostgreSQL migration strategy..."]
```

**Key Features**:
- ✅ Semantic search (understands meaning, not just keywords)
- ✅ Agent identity isolation (`agent_id="sam"` → separate memory pool)
- ✅ Persistent across sessions (survives restarts)
- ✅ Fast queries (<200ms typical)
- ✅ Negligible cost (~$0.02/year for casual users)

### What V1.0 Explicitly Does NOT Deliver

**Deferred to V1.5** (if users complain):
- ❌ Automatic memory injection (Context module)
- ❌ Scratchpad auto-loading
- ❌ Sophisticated ranking algorithms

**Deferred to V2.0** (if users complain):
- ❌ Automatic memory capture (Hook module)
- ❌ Cross-agent memory sharing
- ❌ Access tracking and boosting
- ❌ Context sink delegation pattern
- ❌ Complex analytics

### Why This Approach

**1. Cost is Not a Factor**

| Memories | Annual Embedding Cost |
|----------|----------------------|
| 1,000 | $0.02 |
| 5,000 | $0.10 |
| 10,000 | $0.20 |
| 50,000 | $1.00 |

Vector-only from day 1 costs pennies. Don't build migration tooling to save $0.08/year.

**2. Simplicity Wins**

- **Current over-engineered design**: 5,000+ lines, 10-12 weeks
- **V1.0 ruthless design**: 500 lines, 1-3 days
- **Core value delivered**: Identical (semantic memory with agent isolation)

**3. Build for Reality, Not Theory**

You have **zero memories** today. Don't build infrastructure for 5,000 memories you don't have. Ship fast, learn from real usage, iterate based on actual pain points.

**4. Eliminate Migration Complexity**

Single backend = no migration engine = no data loss risk = 30% less effort in critical phase.

---

## 2. Architecture (Ruthlessly Simple)

### Directory Structure

```
amplifier-bundle-agent-memory/
├── bundle.yaml              (~20 lines)
├── README.md
└── modules/
    └── tool-memory-semantic/
        ├── __init__.py      (~10 lines)
        ├── models.py        (~50 lines)   # Memory data model
        ├── embeddings.py    (~50 lines)   # OpenAI embedding wrapper
        ├── storage.py       (~200 lines)  # Qdrant client wrapper
        └── tools.py         (~200 lines)  # Two tools + registration

Total: ~530 lines including comments and error handling
```

### Module Responsibilities

**models.py**: Single Pydantic model
```python
class Memory(BaseModel):
    id: str
    agent_id: str
    content: str
    timestamp: datetime
    embedding: list[float]
    tags: list[str] = []
```

**embeddings.py**: OpenAI API wrapper
```python
async def create_embedding(text: str) -> list[float]:
    """Generate embedding vector for text."""
```

**storage.py**: Qdrant embedded client
```python
class MemoryStorage:
    async def store(memory: Memory) -> None
    async def search(query_embedding: list[float], limit: int) -> list[Memory]
    async def get_recent(agent_id: str, limit: int) -> list[Memory]
```

**tools.py**: Two Amplifier tools
```python
async def memory_store(content: str, tags: list[str] = []) -> str
async def memory_search(query: str, limit: int = 5, since: datetime = None) -> list[Memory]
```

### No Abstractions

- No storage backend interface (single implementation)
- No migration engine (single backend)
- No cache layer (query vector DB directly)
- No complex state management (stateless tools)

---

## 3. Data Model (Minimal)

### Memory Schema

```python
from datetime import datetime
from pydantic import BaseModel, Field
from uuid import uuid4

class Memory(BaseModel):
    """A single memory entry with semantic embedding."""
    
    id: str = Field(
        default_factory=lambda: f"mem-{uuid4().hex[:8]}",
        description="Unique memory identifier"
    )
    
    agent_id: str = Field(
        description="Agent namespace (e.g., 'sam', 'project-alpha')"
    )
    
    content: str = Field(
        description="The actual memory content (human-readable text)"
    )
    
    timestamp: datetime = Field(
        default_factory=datetime.utcnow,
        description="When this memory was created (UTC)"
    )
    
    embedding: list[float] = Field(
        description="OpenAI text-embedding-3-small vector (1536 dimensions)"
    )
    
    tags: list[str] = Field(
        default=[],
        description="Optional tags for categorization"
    )
```

### What's NOT in the Model

**Deliberately excluded** (can add later if needed):

- ❌ `access_count: int` - No access tracking in V1.0
- ❌ `last_accessed: datetime` - No access boosting
- ❌ `category: str` - No enforced categorization
- ❌ `metadata: dict` - No generic metadata blob
- ❌ `source: str` - No provenance tracking
- ❌ `importance: float` - No importance scoring

### Rationale

Every field costs:
- Embedding dimensions (for searchable fields)
- Storage bytes
- Code complexity (validation, migration, serialization)

**Start minimal. Add fields when users complain about missing capabilities.**

---

## 4. Storage Layer (Single Backend)

### Qdrant Embedded Mode

**Configuration**:
```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

# Storage location
storage_path = Path.home() / ".amplifier" / "memory" / agent_id / "qdrant.db"

# Initialize client
client = QdrantClient(path=str(storage_path))

# Create collection (one-time setup)
client.create_collection(
    collection_name="memories",
    vectors_config=VectorParams(
        size=1536,  # text-embedding-3-small
        distance=Distance.COSINE
    )
)
```

### Why Qdrant Embedded?

| Requirement | Qdrant Embedded | Alternatives |
|-------------|----------------|--------------|
| No separate server | ✅ Single file | ❌ ChromaDB needs server |
| Fast queries | ✅ <100ms | ❌ SQLite FTS slower |
| Semantic search | ✅ Native vectors | ❌ SQLite needs extensions |
| Agent isolation | ✅ Separate DB files | ⚠️ Requires multi-tenancy |
| Setup complexity | ✅ pip install qdrant-client | ❌ Weaviate needs Docker |
| Proven at scale | ✅ 100k+ vectors | ⚠️ Varies by tool |

### Storage Paths

```
~/.amplifier/memory/
├── sam/
│   └── qdrant.db          # Sam's memory database
├── alice/
│   └── qdrant.db          # Alice's memory database
└── project-alpha/
    └── qdrant.db          # Project-specific memory
```

**Agent namespace isolation** = security boundary. No shared state between agents.

### No File Storage

**Original design had**:
```
~/.amplifier/memory/{agent_id}/
├── memories.yaml          # File storage (0-5k memories)
├── qdrant.db              # Vector storage (5k+ memories)
└── .migration_state.json  # Migration tracking
```

**V1.0 simplification**:
```
~/.amplifier/memory/{agent_id}/
└── qdrant.db              # Vector storage (all memories)
```

**Eliminated**:
- ❌ Dual storage backends
- ❌ Migration engine (backup, validate, rollback)
- ❌ File→Vector threshold detection
- ❌ Testing matrix explosion (2 backends × 3 states)

**Result**: 50% fewer code paths, 30% less implementation time, zero data loss risk.

---

## 5. Two Tools (Essential Only)

### Tool 1: memory_store()

**Purpose**: Store a memory with semantic embedding.

```python
async def memory_store(
    content: str,
    tags: list[str] = []
) -> str:
    """
    Store a memory for the current agent.
    
    Args:
        content: The memory text (human-readable description)
        tags: Optional tags for organization (e.g., ["database", "architecture"])
    
    Returns:
        Memory ID (e.g., "mem-a3f2b8c1")
    
    Example:
        >>> memory_store("Decided to use PostgreSQL for user data", tags=["database"])
        "mem-a3f2b8c1"
    """
```

**Implementation**:
1. Get `agent_id` from session context
2. Validate `content` (non-empty, <10k chars)
3. Generate embedding via OpenAI API
4. Create `Memory` object
5. Store in Qdrant
6. Return memory ID

**Error Handling**:
- Empty content → ValueError
- Embedding API failure → Retry 3x, then raise
- Storage failure → Raise with details

### Tool 2: memory_search()

**Purpose**: Search memories semantically.

```python
async def memory_search(
    query: str,
    limit: int = 5,
    since: Optional[datetime] = None
) -> list[Memory]:
    """
    Search memories using semantic similarity.
    
    Args:
        query: Natural language search query
        limit: Maximum number of results (default 5, max 20)
        since: Optional datetime filter (only return memories after this time)
    
    Returns:
        List of Memory objects, ranked by relevance
    
    Example:
        >>> memory_search("what did we do about databases?")
        [
            Memory(content="Decided to use PostgreSQL...", timestamp=...),
            Memory(content="Discussed sharding strategy...", timestamp=...)
        ]
    """
```

**Implementation**:
1. Get `agent_id` from session context
2. Validate `query` (non-empty)
3. Generate query embedding via OpenAI API
4. Search Qdrant with cosine similarity
5. Apply recency boost (see Section 6)
6. Filter by `since` if provided
7. Return top `limit` results

**Error Handling**:
- Empty query → ValueError
- Embedding API failure → Retry 3x, then raise
- Storage failure → Raise with details

### What's NOT Included

**Deferred operations** (can add in V1.5+ if needed):

- ❌ `memory_list_recent()` - Just use `memory_search()` with recent time filter
- ❌ `memory_delete()` - Rare need, can add later
- ❌ `memory_update()` - Memories are immutable in V1.0
- ❌ `memory_promote()` - No file→vector migration needed
- ❌ `memory_export()` - Nice-to-have, not essential
- ❌ `memory_stats()` - Analytics for V2.0

**Rationale**: Two tools cover 95% of use cases. Add operations when users complain about specific pain points.

---

## 6. Ranking (Simple)

### Scoring Algorithm

```python
def rank_results(results: list[ScoredMemory]) -> list[Memory]:
    """
    Rank search results by similarity + recency boost.
    
    Simple algorithm:
    - Base score = cosine similarity (0.0 to 1.0)
    - Recency boost = 1.2× if memory is <7 days old
    - Final score = base_score × recency_factor
    """
    for result in results:
        base_score = result.score  # Qdrant cosine similarity
        
        # Recency boost
        days_old = (datetime.utcnow() - result.memory.timestamp).days
        recency_factor = 1.2 if days_old < 7 else 1.0
        
        result.final_score = base_score * recency_factor
    
    # Sort by final score descending
    return sorted(results, key=lambda x: x.final_score, reverse=True)
```

### What's NOT in Ranking

**Original over-engineered design**:
```python
# Multi-factor ranking with tunable coefficients
freshness_factor = 1.0 / (1.0 + 0.1 * age_days)
access_boost = 1.0 + (0.05 * access_count)
tag_match_bonus = 1.15 if any(tag in query for tag in memory.tags) else 1.0
final_score = similarity * freshness_factor * access_boost * tag_match_bonus
```

**V1.0 simplification**:
```python
# Simple recency boost
score = similarity * (1.2 if days_old < 7 else 1.0)
```

**Eliminated complexity**:
- ❌ Freshness decay coefficient (0.1) - no data to tune this
- ❌ Access count tracking - requires infrastructure
- ❌ Access boost factor (0.05) - marginal value
- ❌ Tag matching bonus - query embedding already captures this
- ❌ Multi-factor formula - 4× more complex, unproven benefit

**Rationale**:
- Cosine similarity is 90% of ranking quality
- Simple recency boost handles "what did I do recently?" queries
- Can tune coefficients when you have 1,000+ memories and real usage data

### When to Add Sophisticated Ranking

**Triggers for V2.0**:
1. **Have corpus**: >1,000 memories per user
2. **Have failure data**: Users report "wrong results" for specific queries
3. **Can A/B test**: Compare simple vs. sophisticated ranking with real queries
4. **Measure improvement**: Quantify ranking quality gains

**Don't tune algorithms you can't measure.**

---

## 7. Agent Identity Model

### Session Capability

Agent identity is a **session-level capability** configured when starting Amplifier:

```yaml
# ~/.amplifier/sessions/sam-dev.yaml
session:
  name: "Sam's Development Session"
  capabilities:
    agent_identity: "sam"  # This session IS sam
```

### How Tools Access Identity

```python
# Inside tool implementation
from amplifier.context import get_current_session

async def memory_store(content: str, tags: list[str] = []) -> str:
    session = get_current_session()
    agent_id = session.capabilities.get("agent_identity")
    
    if not agent_id:
        raise ValueError("No agent_identity configured for this session")
    
    # Use agent_id for namespace isolation
    storage = MemoryStorage(agent_id=agent_id)
    # ...
```

### Namespace Isolation

**Security model**: Each agent_id gets isolated storage.

```python
# Path validation (prevent directory traversal)
def validate_agent_id(agent_id: str) -> None:
    """Ensure agent_id is safe for filesystem paths."""
    if not re.match(r'^[a-z0-9\-_]+$', agent_id):
        raise ValueError(f"Invalid agent_id: {agent_id}")
    
    if '..' in agent_id or '/' in agent_id:
        raise ValueError(f"agent_id cannot contain path separators: {agent_id}")
```

**Storage path construction**:
```python
def get_storage_path(agent_id: str) -> Path:
    """Get isolated storage path for agent."""
    validate_agent_id(agent_id)
    base = Path.home() / ".amplifier" / "memory"
    path = base / agent_id / "qdrant.db"
    path.parent.mkdir(parents=True, exist_ok=True)
    return path
```

### Multi-Tenant Use Cases

**Personal agents**:
```yaml
agent_identity: "sam"        # Sam's personal memory
agent_identity: "alice"      # Alice's personal memory
```

**Project-specific agents**:
```yaml
agent_identity: "project-alpha"   # Project-specific memory
agent_identity: "client-acme"     # Client-specific memory
```

**Role-based agents**:
```yaml
agent_identity: "architect"       # Architecture decisions
agent_identity: "reviewer"        # Code review notes
```

### No Cross-Agent Sharing in V1.0

**Deferred to V2.0**:
- ❌ Shared memory pools
- ❌ Cross-agent queries
- ❌ Memory permissions/ACLs
- ❌ Agent collaboration

**V1.0 isolation model**: Each agent sees only their own memories. Simple, secure, sufficient.

---

## 8. What's Deferred to V1.5/V2.0

### V1.5: Context Module (If Users Complain)

**Trigger**: "Tired of calling memory_search explicitly in every session"

**Feature**: Scratchpad auto-injection

```python
# Context module adds this automatically
async def on_session_start(session: Session):
    """Load recent memories into session context."""
    recent_memories = await memory_search("", limit=10, since=7_days_ago)
    session.context["scratchpad"] = recent_memories
```

**Implementation**: 1-2 days
**LOC**: +150 lines

**User experience**:
```
# WITHOUT context module (V1.0)
User: "What did we decide about databases?"
Agent: *calls memory_search()* "We decided to use PostgreSQL..."

# WITH context module (V1.5)  
User: "What did we decide about databases?"
Agent: *checks pre-loaded scratchpad* "We decided to use PostgreSQL..."
```

**Value**: Faster responses, no explicit tool calls for recent memories.

**Don't build until**: Users complain that explicit queries are tedious.

---

### V2.0: Hook Module (If Users Complain)

**Trigger**: "I keep forgetting to store important decisions"

**Feature**: Automatic memory capture

```python
# Hook module observes events
@hook.on("tool:after_execute")
async def capture_tool_results(event):
    """Auto-store tool results that look important."""
    if is_worth_remembering(event.result):
        await memory_store(summarize(event.result))

@hook.on("turn:end")
async def capture_decisions(event):
    """Auto-store decisions from conversation."""
    if contains_decision(event.turn):
        await memory_store(extract_decision(event.turn))
```

**Implementation**: 3-5 days (needs LLM classification)
**LOC**: +300 lines

**Complexity**:
- Pattern matching (what's worth remembering?)
- False positives (storing noise)
- User control (opt-in/opt-out)
- Background processing (don't block conversation)

**Value**: Hands-free memory capture, less cognitive load.

**Don't build until**: Users complain about manual storage being tedious AND you have data showing they forget important memories.

---

### V2.0: Sophisticated Ranking (If Data Justifies)

**Trigger**: "Search results aren't relevant for my queries"

**Feature**: Multi-factor ranking with tuned coefficients

```python
# Sophisticated ranking
freshness_factor = 1.0 / (1.0 + DECAY_COEFFICIENT * age_days)
access_boost = 1.0 + (ACCESS_WEIGHT * access_count)
final_score = similarity * freshness_factor * access_boost
```

**Implementation**: 2-3 days (requires A/B testing infrastructure)
**LOC**: +200 lines

**Prerequisites**:
- Real corpus (1,000+ memories)
- Failure data (queries with poor results)
- Evaluation dataset (ground truth queries → expected results)
- A/B testing framework

**Value**: Better search quality for complex queries.

**Don't build until**: You can measure improvement with real data.

---

### V2.0+: Advanced Features

**Context Sink Pattern** (Power user optimization):
```python
# Delegate complex queries to specialized agent
memory_sink = ContextSink(agent="memory-retrieval")
results = await memory_sink.query("complex natural language query")
```

**Cross-Agent Memory Sharing**:
```python
# Share memories across agents
await memory_share(memory_id, to_agent="alice")
```

**Memory Analytics**:
```python
# Usage statistics
stats = await memory_stats()
# {
#   "total_memories": 1523,
#   "queries_per_day": 8.3,
#   "storage_size_mb": 4.2,
#   "avg_query_latency_ms": 87
# }
```

**Memory Export/Import**:
```python
# Backup and restore
await memory_export("backup.json")
await memory_import("backup.json")
```

**Don't build any of these until users specifically request them.**

---

## 9. Cost Analysis (Why Vector-Only Works)

### Embedding Cost Breakdown

**OpenAI text-embedding-3-small pricing**: $0.020 per 1M tokens (~750k words)

| User Type | Memories/Month | Words/Memory | Monthly Cost | Annual Cost |
|-----------|----------------|--------------|--------------|-------------|
| **Casual User** | 12 | 50 | <$0.001 | $0.01 |
| **Regular Developer** | 150 | 75 | $0.002 | $0.02 |
| **Power User** | 650 | 100 | $0.008 | $0.10 |

### Storage Cost Breakdown

**Qdrant embedded storage**: ~1MB per 10k memories (with 1536-dim vectors)

| Memories | Disk Space | Cost (at $0.10/GB/month) |
|----------|-----------|-------------------------|
| 1,000 | 100 KB | $0.00001/month |
| 5,000 | 500 KB | $0.00005/month |
| 10,000 | 1 MB | $0.0001/month |
| 50,000 | 5 MB | $0.0005/month |

**Storage cost is effectively zero.**

### Query Cost

**Search queries are FREE** - no API calls, just local vector similarity computation.

Latency: <100ms for typical corpus (<10k memories).

### Total Cost of Ownership

**Realistic scenario**: Regular developer, 150 memories/month, using for 1 year

```
Embedding cost:    $0.02/year
Storage cost:      $0.0001/year
Query cost:        $0.00/year
─────────────────────────────────
Total:             $0.02/year
```

**Cost is not a factor. Simplicity wins.**

### Why NOT to Build Migration System

**Original justification**: "File storage is free, vector storage costs money"

**Reality**: 
- Vector storage costs $0.02/year for regular users
- Migration engine development: 2-3 days of engineering time
- Migration engine maintenance: 10%+ of future bug fixes
- Data loss risk: Medium (migration failures)

**Cost to save $0.02/year**:
- 20 hours engineering time @ $150/hr = $3,000
- Ongoing maintenance = $500+/year in debugging time

**Return on investment**: -149,900%

**Conclusion**: Skip file storage entirely. Vector-only from day 1.

---

## 10. Implementation Validation

### Success Criteria for V1.0

**Functional Requirements**:

| Requirement | Test | Passing Criteria |
|-------------|------|------------------|
| ✅ **Store memories** | `memory_store("test content")` | Returns memory ID |
| ✅ **Search semantically** | `memory_search("what about databases?")` | Finds "PostgreSQL migration" |
| ✅ **Agent isolation** | Store as "sam", query as "alice" | Returns empty |
| ✅ **Persistence** | Restart session, query again | Finds stored memories |
| ✅ **Recency boost** | Store recent + old, search | Recent ranked higher |

**Performance Requirements**:

| Metric | Target | Test Method |
|--------|--------|-------------|
| ✅ **Query latency** | <200ms | Measure 100 queries on 1k corpus |
| ✅ **Storage latency** | <500ms | Measure 100 stores (includes embedding) |
| ✅ **Memory overhead** | <50MB | Process memory after 1k memories loaded |
| ✅ **Disk usage** | <2MB | Check DB file size with 10k memories |

**Reliability Requirements**:

| Requirement | Test | Passing Criteria |
|-------------|------|------------------|
| ✅ **No data loss** | Store 100, query 100 times | All memories retrievable |
| ✅ **Embedding failure** | Mock API error, retry | Succeeds after retry |
| ✅ **Invalid agent ID** | Try path traversal attack | Raises ValueError |
| ✅ **Concurrent queries** | 10 parallel searches | All return results |

### When to Move to V1.5

**Signals that Context module is needed**:

1. **User feedback**: ≥3 users complain "I'm tired of calling memory_search in every session"
2. **Usage data**: >50% of sessions start with memory_search call
3. **Pain point**: Users repeatedly ask "what was I working on?"

**Don't build until you see these signals.**

### When to Move to V2.0

**Signals that Hook module is needed**:

1. **User feedback**: ≥3 users complain "I keep forgetting to store important decisions"
2. **Usage data**: Users have <30% memory coverage (many unmemorable conversations)
3. **Pain point**: Manually calling memory_store feels tedious

**Don't build until you see these signals.**

### Real-World Validation Plan

**Week 1**: Deploy to self
- Install bundle
- Use for all development sessions
- Track: How often do I query? How often do I store?

**Week 2-4**: Daily usage
- Note: Do I forget to store important things?
- Note: Are results relevant when I search?
- Note: Do I wish scratchpad was auto-loaded?

**Week 5**: Analyze usage
```bash
# Query logs
$ memory_stats
Total memories: 127
Queries per day: 3.2
Storage per day: 4.5
Top query types: "what did we...", "remind me about...", "where is..."
```

**Week 6**: Decide next steps
- If scratchpad would save time → Build V1.5 Context module
- If forgetting to store is painful → Build V2.0 Hook module
- If search quality is poor → Tune ranking
- If everything works → Ship V1.0 to users

**Don't speculate on what features you need. Use it. Measure. Decide.**

---

## Implementation Checklist

### Day 1: Storage Infrastructure

- [ ] Create project structure
- [ ] Install dependencies (`qdrant-client`, `openai`, `pydantic`)
- [ ] Implement `models.py` (Memory class)
- [ ] Implement `embeddings.py` (OpenAI wrapper)
- [ ] Implement `storage.py` (Qdrant client)
- [ ] Write unit tests for storage layer
- [ ] **Deliverable**: Can store and retrieve memories by ID

### Day 2: Search Tools

- [ ] Implement `memory_store()` tool in `tools.py`
- [ ] Implement `memory_search()` tool in `tools.py`
- [ ] Implement simple recency boost ranking
- [ ] Add agent ID validation
- [ ] Add error handling (retries, validation)
- [ ] Write integration tests
- [ ] **Deliverable**: Working store/search via tool calls

### Day 3: Bundle Packaging

- [ ] Create `bundle.yaml` manifest
- [ ] Write `README.md` with examples
- [ ] Add `__init__.py` with tool registration
- [ ] Test installation in clean environment
- [ ] Write usage documentation
- [ ] **Deliverable**: Installable bundle

### Shipping Checklist

- [ ] V1.0 passes all success criteria (Section 10)
- [ ] Documentation covers common use cases
- [ ] No known data loss bugs
- [ ] Tested with 1,000+ memories
- [ ] Query latency <200ms on realistic corpus

---

## Appendix: Complexity Comparison

### What We Eliminated

| Component | LOC Saved | Risk Reduced |
|-----------|-----------|--------------|
| File storage backend | -800 lines | No file I/O bugs |
| Migration engine | -1,200 lines | No data loss risk |
| Complex ranking | -400 lines | No coefficient tuning |
| Context module | -600 lines | Simpler mental model |
| Hook module | -1,000 lines | No event complexity |
| Cache layer | -300 lines | No cache invalidation |
| Analytics | -400 lines | No metrics infrastructure |

**Total savings**: 4,700 lines not written, 7 major components deferred.

### What We Kept

| Component | LOC | Value |
|-----------|-----|-------|
| Memory model | 50 | Core data structure |
| Embeddings wrapper | 50 | API abstraction |
| Storage layer | 200 | Persistence + search |
| Two tools | 200 | User interface |

**Total implementation**: 500 lines, 4 components.

**Value delivered**: Semantic memory with agent isolation - the entire point of the system.

---

## Final Philosophy Check

### Ruthless Simplicity ✅

- **KISS principle**: 500 lines vs. 5,000+ lines
- **Minimal abstractions**: Single storage backend, no interfaces
- **Start minimal**: Two tools, can add more later
- **No future-proofing**: Built for zero memories, scales to millions

### Architectural Integrity ✅

- **Correct pattern**: Tool module (Amplifier convention)
- **Simple implementation**: Direct Qdrant calls, no layers
- **Clean interfaces**: Two functions, clear contracts
- **Agent namespacing**: Security through isolation

### Library vs Custom ✅

- **Qdrant**: Solves complex vector search (right choice)
- **OpenAI**: Proven embeddings (right choice)
- **Custom ranking**: Simple recency boost (right for V1.0)
- **No heavy frameworks**: Direct API usage

### Decision Framework ✅

1. **Necessity**: Do we need file storage? NO (cost negligible)
2. **Simplicity**: What's simplest backend? Vector-only
3. **Directness**: Can we skip migration? YES
4. **Value**: Does complexity help? NO (unproven algorithms)
5. **Maintenance**: Easy to understand? YES (500 lines, 1 backend)

---

## Ship It

**This design is READY FOR IMPLEMENTATION.**

- ✅ Ruthlessly simple (~500 lines)
- ✅ Delivers core value (semantic memory + agent isolation)
- ✅ Eliminates biggest risks (migration, dual backends)
- ✅ Defers unproven features (hooks, context, complex ranking)
- ✅ 1-3 days implementation time
- ✅ Can iterate based on real usage

**Stop planning. Start building. Learn from reality.**

---

**Next Step**: Hand off to `modular-builder` agent for implementation.

**Specification completeness**: ✅ COMPLETE
- [x] Data sources: Qdrant embedded, OpenAI embeddings API
- [x] Error handling: Retries, validation, clear errors
- [x] Dependencies: qdrant-client, openai, pydantic
- [x] Integration: Tool registration, session context
- [x] Examples: All tools demonstrated with code
- [x] Constraints: <200ms queries, <500ms storage, ~500 LOC

**Ready for handoff**: YES
