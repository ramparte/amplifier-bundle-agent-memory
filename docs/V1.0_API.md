# V1.0 API Reference (Ruthlessly Simple)

**Version**: 1.0.0  
**Status**: Ready for implementation  
**Complexity**: ~500 lines total

---

## Tool APIs

### memory_store()

Store a memory with semantic embedding.

**Signature**:
```python
async def memory_store(
    content: str,
    tags: list[str] = []
) -> str
```

**Parameters**:
| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `content` | string | Yes | Memory content (1-10,000 chars) |
| `tags` | array[string] | No | Optional tags for filtering |

**Returns**: `string` - Memory ID (e.g., "mem-a3f2b8c1")

**Example**:
```python
# Store simple memory
memory_id = await memory_store(
    content="Decided to use PostgreSQL for the new project database"
)

# Store with tags
memory_id = await memory_store(
    content="API rate limit is 1000 requests/hour",
    tags=["api", "limits", "infra"]
)
```

**Tool Result Format**:
```json
{
    "success": true,
    "data": {
        "memory_id": "mem-a3f2b8c1",
        "embedding_cost": 0.00002
    }
}
```

**Error Handling**:
- Empty content → ValidationError
- Embedding API failure → Retry 3x, then fail gracefully
- Agent identity missing → Error with clear message

**Performance**: ~300ms (embedding generation dominates)

---

### memory_search()

Search memories semantically using natural language.

**Signature**:
```python
async def memory_search(
    query: str,
    limit: int = 5,
    since: Optional[datetime] = None
) -> list[Memory]
```

**Parameters**:
| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `query` | string | Yes | Natural language search query |
| `limit` | integer | No | Max results (default: 5, max: 20) |
| `since` | datetime | No | Only memories after this timestamp |

**Returns**: `list[Memory]` - Ranked by relevance

**Memory Object**:
```python
{
    "id": "mem-a3f2b8c1",
    "content": "Decided to use PostgreSQL...",
    "timestamp": "2026-01-15T14:30:00Z",
    "tags": ["database", "decisions"],
    "relevance_score": 0.87
}
```

**Example**:
```python
# Simple search
results = await memory_search("database decisions")

# Recent memories only
from datetime import datetime, timedelta
last_week = datetime.utcnow() - timedelta(days=7)
results = await memory_search(
    query="what API work did I do?",
    limit=10,
    since=last_week
)
```

**Tool Result Format**:
```json
{
    "success": true,
    "data": {
        "memories": [
            {
                "id": "mem-a3f2b8c1",
                "content": "Decided to use PostgreSQL...",
                "timestamp": "2026-01-15T14:30:00Z",
                "tags": ["database"],
                "relevance_score": 0.87
            }
        ],
        "query_time_ms": 87
    }
}
```

**Ranking Algorithm** (Simple):
```python
# Base score from vector similarity (0.0-1.0)
score = cosine_similarity(query_embedding, memory_embedding)

# Recency boost for last 7 days
if (now - memory.timestamp).days < 7:
    score *= 1.2

# Sort by final score
```

**Performance**: <200ms for typical corpus (1k-5k memories)

---

## Python Module APIs (For Developers)

### MemoryStorage

Main storage interface wrapping Qdrant.

```python
from memory_semantic import MemoryStorage

storage = MemoryStorage(agent_id="sam")

# Store
memory_id = await storage.store(
    content="Memory content",
    tags=["tag1", "tag2"]
)

# Search
results = await storage.search(
    query="natural language query",
    limit=5
)

# Get by ID
memory = await storage.get(memory_id)
```

**Methods**:
- `async store(content: str, tags: list[str]) -> str`
- `async search(query: str, limit: int, since: datetime) -> list[Memory]`
- `async get(memory_id: str) -> Optional[Memory]`

**Storage Location**: `~/.amplifier/memory/{agent_id}/qdrant.db`

---

### EmbeddingGenerator

Wrapper for OpenAI embedding API.

```python
from memory_semantic import EmbeddingGenerator

embedder = EmbeddingGenerator()

# Single embedding
embedding = await embedder.generate("text to embed")
# Returns: list[float] with 1536 dimensions

# Batch embeddings (for efficiency)
embeddings = await embedder.generate_batch([
    "text 1",
    "text 2",
    "text 3"
])
```

**Configuration**:
```python
embedder = EmbeddingGenerator(
    model="text-embedding-3-small",  # Default
    api_key=None  # Uses OPENAI_API_KEY env var
)
```

**Cost**: ~$0.00002 per memory (~$0.10 for 5,000 memories)

---

## Configuration

### Session Configuration

Enable memory for a session by setting `agent_identity` capability:

**settings.yaml**:
```yaml
session:
  capabilities:
    agent_identity: "sam"  # Your agent identity
```

**Natural language** (in session):
```
User: "Start as sam"
Agent: [Recognizes pattern, sets agent_identity]
```

**Programmatic**:
```python
session = AmplifierSession(
    capabilities={"agent_identity": "sam"}
)
```

### Tool Module Configuration

**Bundle configuration** (optional):
```yaml
tools:
  - module: tool-memory-semantic
    config:
      embedding_model: "text-embedding-3-small"  # Default
      max_query_results: 20  # Max limit parameter
      storage_root: "~/.amplifier/memory"  # Default
```

---

## Data Models

### Memory

```python
from pydantic import BaseModel, Field
from datetime import datetime
from uuid import uuid4

class Memory(BaseModel):
    id: str = Field(default_factory=lambda: f"mem-{uuid4().hex[:8]}")
    agent_id: str
    content: str
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    embedding: list[float]  # 1536 dimensions
    tags: list[str] = []
```

**Validation Rules**:
- `content`: 1-10,000 characters
- `agent_id`: Alphanumeric + hyphens/underscores only (security)
- `embedding`: Must be exactly 1536 dimensions
- `tags`: Max 20 tags per memory

**Serialization**:
```python
# To dict
memory_dict = memory.dict()

# From dict
memory = Memory(**memory_dict)

# JSON
import json
json_str = memory.json()
memory = Memory.parse_raw(json_str)
```

---

## Integration Examples

### Basic Usage

```python
# In an Amplifier session with agent_identity set

# Store memory
result = await invoke_tool("memory_store", {
    "content": "PostgreSQL is our standard for new projects",
    "tags": ["database", "standards"]
})
# Returns: {"memory_id": "mem-a3f2b8c1"}

# Search memory
results = await invoke_tool("memory_search", {
    "query": "what's our database standard?",
    "limit": 3
})
# Returns: [Memory(...), Memory(...), Memory(...)]
```

### Natural Language Usage

```
User: "Remember this: we use PostgreSQL for all new projects"
Agent: [Calls memory_store internally]
      ✓ Stored memory: mem-a3f2b8c1

User: "What's our database standard?"
Agent: [Calls memory_search("database standard")]
      Based on your previous decision, we use PostgreSQL for all new projects.
```

### Bundle Inclusion

```yaml
# your-bundle/bundle.md
includes:
  - bundle: git+https://github.com/USER/amplifier-bundle-agent-memory@main

session:
  capabilities:
    agent_identity: "sam"
```

---

## Error Handling

### ValidationError

```python
try:
    await memory_store(content="")
except ValidationError as e:
    # Empty content not allowed
    print(e.errors())
```

### EmbeddingError

```python
try:
    await memory_store(content="Valid content")
except EmbeddingError as e:
    # OpenAI API failure (retried 3x)
    print(f"Embedding generation failed: {e}")
```

### StorageError

```python
try:
    await memory_search("query")
except StorageError as e:
    # Qdrant database issue
    print(f"Storage access failed: {e}")
```

### MissingAgentIdentity

```python
# If agent_identity capability not set
try:
    await memory_store("content")
except MissingAgentIdentity:
    print("Memory requires agent_identity capability")
```

**Error Recovery**:
- Embedding API failures → Retry 3x with exponential backoff
- Qdrant connection issues → Return empty results gracefully
- Validation errors → Clear error messages

---

## Events Emitted

### memory:stored

Emitted when a memory is successfully stored.

**Payload**:
```json
{
    "agent_id": "sam",
    "memory_id": "mem-a3f2b8c1",
    "embedding_cost": 0.00002
}
```

### memory:searched

Emitted when a search is performed.

**Payload**:
```json
{
    "agent_id": "sam",
    "query": "database decisions",
    "results_count": 3,
    "query_time_ms": 87
}
```

---

## Performance Characteristics

### Latency Targets

| Operation | Target | Typical |
|-----------|--------|---------|
| memory_store() | <500ms | ~300ms |
| memory_search() | <200ms | ~100ms |
| Embedding generation | <300ms | ~250ms |
| Vector search | <100ms | ~50ms |

### Scalability

| Memories | Storage Size | Query Time |
|----------|-------------|------------|
| 1,000 | ~2 MB | ~50ms |
| 5,000 | ~10 MB | ~75ms |
| 10,000 | ~20 MB | ~100ms |
| 50,000 | ~100 MB | ~150ms |

**Note**: Qdrant's HNSW index ensures O(log n) query complexity.

---

## Cost Breakdown

### OpenAI Embedding API

**Model**: text-embedding-3-small  
**Cost**: $0.00002 per 1k tokens

| Usage Level | Memories/Month | Monthly Cost | Annual Cost |
|-------------|----------------|--------------|-------------|
| Casual | 12 | <$0.01 | ~$0.02 |
| Regular | 150 | ~$0.02 | ~$0.18 |
| Power User | 650 | ~$0.07 | ~$0.78 |

**Conclusion**: Cost is negligible even for power users.

---

## Security & Privacy

### Namespace Isolation

Each agent identity has isolated storage:
```
~/.amplifier/memory/
├── sam/
│   └── qdrant.db
├── alice/
│   └── qdrant.db
└── bob/
    └── qdrant.db
```

**Security measures**:
- Agent ID validation prevents path traversal
- No cross-agent access possible
- All data stays local (no cloud uploads)

**Agent ID Validation**:
```python
def is_valid_agent_id(agent_id: str) -> bool:
    # Alphanumeric, hyphens, underscores only
    return bool(re.match(r'^[a-zA-Z0-9_-]+$', agent_id))

# These fail validation (security)
assert not is_valid_agent_id("../alice")
assert not is_valid_agent_id("sam/../../etc")
```

### Sensitive Data

**Caveat**: Content is sent to OpenAI for embedding generation.

**Recommendations**:
- Don't store API keys or secrets
- Don't store personally identifiable information (PII)
- Review content before storing sensitive data

**Future**: Local embedding models (sentence-transformers) for full privacy (V2.0+).

---

## What's NOT in V1.0

These features are **deliberately deferred** to future versions:

### V1.5 Features (If Users Complain)
- Context module (auto-inject scratchpad at session start)
- Sophisticated ranking algorithms
- `memory_promote()` tool
- `memory_list_recent()` tool

### V2.0 Features (If Users Complain)
- Hook module (auto-capture from tool outputs)
- Access count tracking
- Freshness decay formulas
- Context sink delegation (memory-assistant agent)
- Cross-agent memory sharing

### Never Planned
- File storage backend (eliminated)
- Migration engine (eliminated)
- Complex metadata fields (eliminated)

**Philosophy**: Build minimum viable product, add features when users complain about specific pain points.

---

## Quick Reference

### Two Tools Summary

```python
# Store
memory_store(content: str, tags: list[str] = []) -> str

# Search  
memory_search(query: str, limit: int = 5, since: datetime = None) -> list[Memory]
```

### Memory Object Summary

```python
{
    "id": "mem-a3f2b8c1",
    "agent_id": "sam",
    "content": "Memory text",
    "timestamp": "2026-01-31T15:00:00Z",
    "embedding": [0.123, -0.456, ...],  # 1536 dims
    "tags": ["tag1", "tag2"]
}
```

### Configuration Summary

```yaml
session:
  capabilities:
    agent_identity: "sam"
```

---

## Next Steps

1. **Implement V1.0** (~500 lines, 1-3 days)
2. **Deploy personally** (use for 2-4 weeks)
3. **Measure usage**:
   - How often do you query memory?
   - What types of queries do you run?
   - Do you want auto-features?
4. **Decide V1.5/V2.0** based on real data

**Don't build features until users complain about their absence.**

---

**End of V1.0 API Reference**
